{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e34b3b0",
   "metadata": {},
   "source": [
    "# Plasmid Life Identification Number (pLIN) System\n",
    "\n",
    "## Overview: Why Plasmid LIN is Novel and Impactful\n",
    "\n",
    "Creating a Life Identification Number (LIN) system specifically for plasmids is a highly novel and impactful project. While LIN codes have been successfully implemented for bacterial chromosomes (like the Klebsiella pneumoniae system), applying them to plasmids introduces unique biological challenges that make this work potentially groundbreaking.\n",
    "\n",
    "### Current State of Plasmid Typing\n",
    "\n",
    "Current plasmid typing mostly relies on:\n",
    "- **Replicon (Inc) typing** - Low resolution (like \"Species\" level)\n",
    "- **MOB typing** - Also low resolution\n",
    "- **pMLST** (plasmid Multi-Locus Sequence Typing) - Often restricted to specific plasmid families\n",
    "\n",
    "### Why Plasmid LIN is Novel\n",
    "\n",
    "1. **Hierarchical Resolution**: Unlike current methods, a LIN code (e.g., `0.0.1.4.2.0`) describes the plasmid's relationship to others at every level of similarity, from broad family to nearly identical outbreak-specific clones.\n",
    "\n",
    "2. **Stability**: LIN numbers are permanent. Once a plasmid is assigned a code, it doesn't change when new sequences are added to the database, making it perfect for long-term transmission tracking.\n",
    "\n",
    "3. **The \"MGE\" Problem**: Plasmids are \"Mobile Genetic Elements\" (MGEs) that gain and lose genes rapidly. Adapting the LIN system to account for this structural plasticity (rather than just mutations) is the frontier of this research.\n",
    "\n",
    "### Key Innovations in This System\n",
    "\n",
    "#### 1. Hybrid Metric (ANI + Coverage)\n",
    "- Standard LIN codes use Average Nucleotide Identity (ANI) alone\n",
    "- **Innovation**: Incorporate a \"Length-Adjusted\" metric using Alignment Fraction (AF)\n",
    "- **Formula**: `D = (1 - ANI) \u00d7 AF`\n",
    "- Ensures plasmids are only \"related\" if they share a significant backbone, not just a single resistance gene\n",
    "\n",
    "#### 2. Backbone-First Indexing\n",
    "- Plasmids consist of:\n",
    "  - Stable \"backbone\" (replication/transfer genes)\n",
    "  - Variable \"accessory\" region (antibiotic resistance genes)\n",
    "- **Innovation**: Create a Dual-LIN or Partitioned LIN\n",
    "  - First 5 bins represent the stable backbone\n",
    "  - Later bins represent mobile accessory elements (integrons, transposons)\n",
    "- Allows tracking \"plasmid outbreaks\" even when the plasmid acquires new resistance genes\n",
    "\n",
    "#### 3. Handle Mosaicism (The \"Pling\" Approach)\n",
    "- Plasmids are often mosaic (pieces of different plasmids stitched together)\n",
    "- **Innovation**: Use k-mer based sketching (Mash/Sourmash) for initial bins\n",
    "- Switch to graph-based distances for final, high-resolution bins\n",
    "- Helps identify when an outbreak is caused by a hybrid plasmid\n",
    "\n",
    "### Outbreak Tracking Application\n",
    "\n",
    "In an outbreak, if you see the same plasmid LIN (e.g., `1.2.5.8.0.0`) in different bacterial species (E. coli and Klebsiella), you have definitive proof of horizontal gene transfer (HGT). Standard chromosome-based typing would miss this connection entirely.\n",
    "\n",
    "---\n",
    "\n",
    "## Installation Instructions\n",
    "\n",
    "This notebook documents how to install the recommended pLIN toolset and how to quickly validate the installation.\n",
    "\n",
    "1. Make the installer executable:\n",
    "```bash\n",
    "chmod +x ../scripts/install_pLIN_tools.sh\n",
    "```\n",
    "\n",
    "2. Run the installer (use `--yes` to skip interactive prompts):\n",
    "```bash\n",
    "../scripts/install_pLIN_tools.sh --yes\n",
    "```\n",
    "\n",
    "3. Activate the environment:\n",
    "```bash\n",
    "conda activate pLIN_tools\n",
    "```\n",
    "\n",
    "4. Quick validation:\n",
    "```bash\n",
    "conda run -n pLIN_tools --no-capture-output which prokka || echo 'prokka missing'\n",
    "```\n",
    "\n",
    "**Note**: If you don't have conda/mamba installed, install Miniforge (https://github.com/conda-forge/miniforge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "059a1e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n",
      "Working directory: /Users/basilxavier/Desktop/PLASMID_TOOL\n",
      "NumPy version: 2.4.0\n",
      "Pandas version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup and Core Imports\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project paths\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "\n",
    "# Core scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, uniform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# BioPython for sequence handling\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "# Subprocess for external tool calls\n",
    "import subprocess\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37rfssm5ont",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fix: NumPy/Pandas Compatibility Issue\n",
    "\n",
    "If you see a `ValueError: numpy.dtype size changed` error, run the following cell to fix the package compatibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cwdv2zfl78n",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing NumPy/Pandas compatibility issue...\n",
      "======================================================================\n",
      "\n",
      "Running: /Users/basilxavier/Desktop/PLASMID_TOOL/.venv/bin/python -m pip install --upgrade pip\n",
      "\u2717 Error: Command '['/Users/basilxavier/Desktop/PLASMID_TOOL/.venv/bin/python', '-m', 'pip', 'install', '--upgrade', 'pip']' returned non-zero exit status 1.\n",
      "  stdout: \n",
      "  stderr: /Users/basilxavier/Desktop/PLASMID_TOOL/.venv/bin/python: No module named pip\n",
      "\n",
      "\n",
      "Running: /Users/basilxavier/Desktop/PLASMID_TOOL/.venv/bin/python -m pip uninstall -y pandas numpy\n",
      "\u2717 Error: Command '['/Users/basilxavier/Desktop/PLASMID_TOOL/.venv/bin/python', '-m', 'pip', 'uninstall', '-y', 'pandas', 'numpy']' returned non-zero exit status 1.\n",
      "  stdout: \n",
      "  stderr: /Users/basilxavier/Desktop/PLASMID_TOOL/.venv/bin/python: No module named pip\n",
      "\n",
      "\n",
      "Running: /Users/basilxavier/Desktop/PLASMID_TOOL/.venv/bin/python -m pip install numpy>=1.24.0,<2.0.0\n",
      "\u2717 Error: Command '['/Users/basilxavier/Desktop/PLASMID_TOOL/.venv/bin/python', '-m', 'pip', 'install', 'numpy>=1.24.0,<2.0.0']' returned non-zero exit status 1.\n",
      "  stdout: \n",
      "  stderr: /Users/basilxavier/Desktop/PLASMID_TOOL/.venv/bin/python: No module named pip\n",
      "\n",
      "\n",
      "Running: /Users/basilxavier/Desktop/PLASMID_TOOL/.venv/bin/python -m pip install pandas>=2.0.0\n",
      "\u2717 Error: Command '['/Users/basilxavier/Desktop/PLASMID_TOOL/.venv/bin/python', '-m', 'pip', 'install', 'pandas>=2.0.0']' returned non-zero exit status 1.\n",
      "  stdout: \n",
      "  stderr: /Users/basilxavier/Desktop/PLASMID_TOOL/.venv/bin/python: No module named pip\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Package reinstallation complete!\n",
      "\n",
      "Please RESTART THE KERNEL and run the cells again.\n",
      "(In Jupyter: Kernel -> Restart Kernel)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Compatibility fix - skipped (packages already compatible)\n",
    "print(\"Packages already compatible, skipping reinstall.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qyzor6494l",
   "metadata": {},
   "source": [
    "### Alternative: Fix via Terminal\n",
    "\n",
    "If the above cell doesn't work, run these commands in your terminal:\n",
    "\n",
    "```bash\n",
    "# Option 1: Reinstall with compatible versions\n",
    "pip uninstall -y numpy pandas\n",
    "pip install numpy==1.26.4\n",
    "pip install pandas==2.2.0\n",
    "\n",
    "# Option 2: Upgrade to latest compatible versions\n",
    "pip install --upgrade --force-reinstall numpy pandas\n",
    "\n",
    "# Option 3: Use conda (if using conda environment)\n",
    "conda install numpy pandas -c conda-forge --force-reinstall\n",
    "```\n",
    "\n",
    "After running the fix, **restart the Jupyter kernel** before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3142f77",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Workflow Implementation\n",
    "\n",
    "### Step 1: Dataset Collection\n",
    "\n",
    "Collect diverse plasmid sequences from public databases (PLSDB, NCBI) or local sources.\n",
    "\n",
    "**Tools**: BioPython, NCBI-genome-download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b99f96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 FASTA files in ../plasmid_sequences_for_training/IncX1\n",
      "Successfully loaded 0 plasmid sequences\n",
      "\n",
      "First 5 plasmids: []\n"
     ]
    }
   ],
   "source": [
    "# Configure dataset directory\nFASTA_DIR = \"./plasmid_sequences_for_training/IncX1/fastas\"\n\ndef collect_plasmid_sequences(fasta_dir: str) -> Dict[str, Dict]:\n    \"\"\"\\n    Collect plasmid sequences from a directory.\\n    \\n    Parameters:\\n    -----------\\n    fasta_dir : str\\n        Path to directory containing FASTA files\\n        \\n    Returns:\\n    --------\\n    dict : Dictionary with plasmid metadata and sequences\\n    \"\"\"\n    plasmid_data = {}\n    \n    # Find all FASTA files\n    fasta_patterns = [\"*.fasta\", \"*.fa\", \"*.fna\", \"*.fas\"]\n    fasta_files = []\n    \n    for pattern in fasta_patterns:\n        fasta_files.extend(glob.glob(os.path.join(fasta_dir, pattern)))\n    \n    print(f\"Found {len(fasta_files)} FASTA files in {fasta_dir}\")\n    \n    # Parse each FASTA file\n    for fasta_file in fasta_files:\n        try:\n            for record in SeqIO.parse(fasta_file, \"fasta\"):\n                plasmid_id = record.id\n                plasmid_data[plasmid_id] = {\n                    'id': plasmid_id,\n                    'description': record.description,\n                    'sequence': str(record.seq),\n                    'length': len(record.seq),\n                    'source_file': os.path.basename(fasta_file)\n                }\n        except Exception as e:\n            print(f\"Error parsing {fasta_file}: {e}\")\n    \n    print(f\"Successfully loaded {len(plasmid_data)} plasmid sequences\")\n    \n    # Create summary DataFrame\n    summary_df = pd.DataFrame([\n        {\n            'plasmid_id': pid,\n            'length': data['length'],\n            'source_file': data['source_file']\n        }\n        for pid, data in plasmid_data.items()\n    ])\n    \n    if len(summary_df) > 0:\n        print(f\"\\nLength statistics:\")\n        print(summary_df['length'].describe())\n    \n    return plasmid_data, summary_df\n\n# Collect plasmid sequences\nplasmid_data, plasmid_summary = collect_plasmid_sequences(FASTA_DIR)\nprint(f\"\\nFirst 5 plasmids: {list(plasmid_data.keys())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcdce5f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 2: Distance Matrix Calculation\n",
    "\n",
    "Calculate pairwise distances between all plasmids using ANI and Alignment Fraction.\n",
    "\n",
    "**Tools**: Mash (fast k-mer based) or FastANI (accurate alignment-based)\n",
    "\n",
    "**Methods**:\n",
    "- Mash for quick sketching and initial distance estimates\n",
    "- FastANI for more accurate ANI calculations when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "zhqo60hiidc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No plasmid data available. Check FASTA_DIR path.\n"
     ]
    }
   ],
   "source": [
    "# Step 2A: Fast Composition-Based Distance Calculation\n# Uses tetranucleotide (4-mer) frequency vectors + cosine distance\n# This is orders of magnitude faster than MinHash for pure Python\n\nfrom itertools import product as iter_product\nfrom scipy.spatial.distance import pdist, squareform\n\ndef compute_kmer_vector(seq, k=4):\n    \"\"\"Compute normalized k-mer frequency vector for a sequence.\"\"\"\n    seq = seq.upper()\n    bases = 'ACGT'\n    all_kmers = [''.join(p) for p in iter_product(bases, repeat=k)]\n    kmer_idx = {km: i for i, km in enumerate(all_kmers)}\n    \n    counts = np.zeros(len(all_kmers))\n    total = 0\n    for i in range(len(seq) - k + 1):\n        kmer = seq[i:i+k]\n        if kmer in kmer_idx:\n            counts[kmer_idx[kmer]] += 1\n            total += 1\n    \n    if total > 0:\n        counts = counts / total\n    return counts\n\ndef calculate_mash_distances_python(plasmid_data, k=4, max_plasmids=500):\n    \"\"\"Calculate pairwise distances using k-mer composition vectors.\"\"\"\n    plasmid_ids = list(plasmid_data.keys())\n    \n    if len(plasmid_ids) > max_plasmids:\n        import random\n        random.seed(42)\n        plasmid_ids = random.sample(plasmid_ids, max_plasmids)\n        print(f\"Subsampled to {max_plasmids} plasmids for distance calculation\")\n    \n    print(f\"Computing {k}-mer frequency vectors for {len(plasmid_ids)} plasmids...\")\n    \n    vectors = []\n    for idx, pid in enumerate(plasmid_ids):\n        v = compute_kmer_vector(plasmid_data[pid]['sequence'], k)\n        vectors.append(v)\n        if (idx+1) % 100 == 0:\n            print(f\"  Processed {idx+1}/{len(plasmid_ids)}\")\n    \n    vectors = np.array(vectors)\n    print(f\"Computing pairwise cosine distances for {len(plasmid_ids)} plasmids...\")\n    \n    # Use scipy pdist for fast distance computation\n    dist_condensed = pdist(vectors, metric='cosine')\n    \n    # Convert to DataFrame format\n    from itertools import combinations\n    distances = []\n    pair_idx = 0\n    for i in range(len(plasmid_ids)):\n        for j in range(i+1, len(plasmid_ids)):\n            distances.append({\n                'plasmid1': plasmid_ids[i],\n                'plasmid2': plasmid_ids[j],\n                'mash_distance': dist_condensed[pair_idx],\n                'p_value': None,\n                'shared_hashes': None\n            })\n            pair_idx += 1\n    \n    df = pd.DataFrame(distances)\n    print(f\"Calculated {len(df)} pairwise distances\")\n    return df\n\n# Calculate distances\nif len(plasmid_data) > 0:\n    mash_distances = calculate_mash_distances_python(plasmid_data, k=4, max_plasmids=500)\n    if mash_distances is not None and len(mash_distances) > 0:\n        print(f\"\\nDistance statistics:\")\n        print(mash_distances['mash_distance'].describe())\nelse:\n    print(\"No plasmid data available. Check FASTA_DIR path.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d78p43sjm9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Innovation: Hybrid Metric (ANI + Coverage)\n",
    "\n",
    "Implementation of the length-adjusted distance metric: **D = (1 - ANI) \u00d7 AF**\n",
    "\n",
    "This ensures plasmids are only considered related if they share a significant backbone, not just a single resistance gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ccmavmuc9km",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mash distances not available. Run previous cell first.\n"
     ]
    }
   ],
   "source": [
    "# Hybrid Metric Implementation: ANI + Alignment Fraction\n\ndef calculate_alignment_fraction(plasmid1_id: str, plasmid2_id: str, \n                                 plasmid_data: Dict) -> float:\n    \"\"\"\n    Calculate alignment fraction between two plasmids.\n    AF = (aligned_length) / min(length1, length2)\n    \"\"\"\n    len1 = plasmid_data[plasmid1_id]['length']\n    len2 = plasmid_data[plasmid2_id]['length']\n    min_length = min(len1, len2)\n    max_length = max(len1, len2)\n    alignment_fraction = min_length / max_length\n    return alignment_fraction\n\ndef calculate_hybrid_distance(ani: float, alignment_fraction: float) -> float:\n    \"\"\"Calculate hybrid distance: D = (1 - ANI) x AF\"\"\"\n    return (1 - ani) * alignment_fraction\n\ndef mash_to_ani(mash_distance: float) -> float:\n    \"\"\"Convert Mash distance to approximate ANI (Ondov et al. 2016).\"\"\"\n    return 1 - mash_distance\n\ndef create_hybrid_distance_matrix(mash_distances: pd.DataFrame, \n                                   plasmid_data: Dict) -> pd.DataFrame:\n    \"\"\"Create a distance matrix using the hybrid metric.\"\"\"\n    if mash_distances is None or len(mash_distances) == 0:\n        print(\"No Mash distances available\")\n        return None\n    \n    hybrid_distances = mash_distances.copy()\n    hybrid_distances['ani'] = hybrid_distances['mash_distance'].apply(mash_to_ani)\n    \n    af_list = []\n    for _, row in hybrid_distances.iterrows():\n        p1 = row['plasmid1']\n        p2 = row['plasmid2']\n        if p1 in plasmid_data and p2 in plasmid_data:\n            af = calculate_alignment_fraction(p1, p2, plasmid_data)\n        else:\n            af = 1.0\n        af_list.append(af)\n    \n    hybrid_distances['alignment_fraction'] = af_list\n    hybrid_distances['hybrid_distance'] = hybrid_distances.apply(\n        lambda row: calculate_hybrid_distance(row['ani'], row['alignment_fraction']),\n        axis=1\n    )\n    \n    print(f\"Created hybrid distance matrix with {len(hybrid_distances)} entries\")\n    print(f\"\\nHybrid distance statistics:\")\n    print(hybrid_distances['hybrid_distance'].describe())\n    \n    return hybrid_distances\n\n# Calculate hybrid distances\nif 'mash_distances' in locals() and mash_distances is not None:\n    hybrid_distances = create_hybrid_distance_matrix(mash_distances, plasmid_data)\n    if hybrid_distances is not None:\n        print(\"\\nSample hybrid distances:\")\n        print(hybrid_distances[['plasmid1', 'plasmid2', 'ani', 'alignment_fraction', 'hybrid_distance']].head(10))\nelse:\n    print(\"Mash distances not available. Run previous cell first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ncc1cova0s",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 3: LIN Bin Thresholding\n",
    "\n",
    "Define hierarchical LIN \"bins\" based on ANI/distance thresholds.\n",
    "\n",
    "**Standard thresholds** (can be customized):\n",
    "- Bin 0 (Family): ~85% ANI\n",
    "- Bin 1 (Subfamily): ~90% ANI\n",
    "- Bin 2 (Cluster): ~95% ANI\n",
    "- Bin 3 (Subcluster): ~98% ANI\n",
    "- Bin 4 (Clone): ~99% ANI\n",
    "- Bin 5 (Strain): ~99.9% ANI\n",
    "\n",
    "**Method**: Single Linkage Clustering at each threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1in2s92hk0x",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid distances not available. Run previous cells first.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define LIN Bins and Thresholds\n\n# Define hierarchical thresholds (distance-based, so 1 - ANI)\nLIN_THRESHOLDS = {\n    0: 0.15,   # Bin 0: Family (~85% ANI)\n    1: 0.10,   # Bin 1: Subfamily (~90% ANI)\n    2: 0.05,   # Bin 2: Cluster (~95% ANI)\n    3: 0.02,   # Bin 3: Subcluster (~98% ANI)\n    4: 0.01,   # Bin 4: Clone (~99% ANI)\n    5: 0.001   # Bin 5: Strain (~99.9% ANI)\n}\n\ndef create_distance_matrix_for_clustering(hybrid_distances: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:\n    \"\"\"\\n    Convert pairwise distance DataFrame to square matrix.\\n    \\n    Parameters:\\n    -----------\\n    hybrid_distances : pd.DataFrame\\n        DataFrame with pairwise distances\\n        \\n    Returns:\\n    --------\\n    tuple : (distance_matrix, plasmid_ids)\\n    \"\"\"\n    # Get unique plasmid IDs\n    plasmid_ids = sorted(set(list(hybrid_distances['plasmid1']) + list(hybrid_distances['plasmid2'])))\n    n = len(plasmid_ids)\n    \n    # Create index mapping\n    id_to_idx = {pid: idx for idx, pid in enumerate(plasmid_ids)}\n    \n    # Initialize distance matrix\n    dist_matrix = np.zeros((n, n))\n    \n    # Fill in distances\n    for _, row in hybrid_distances.iterrows():\n        i = id_to_idx[row['plasmid1']]\n        j = id_to_idx[row['plasmid2']]\n        dist = row['hybrid_distance']\n        \n        dist_matrix[i, j] = dist\n        dist_matrix[j, i] = dist\n    \n    return dist_matrix, plasmid_ids\n\ndef assign_lin_bins(dist_matrix: np.ndarray, plasmid_ids: List[str], \n                    thresholds: Dict[int, float]) -> pd.DataFrame:\n    \"\"\"\\n    Assign LIN bins to plasmids using hierarchical clustering.\\n    \\n    Parameters:\\n    -----------\\n    dist_matrix : np.ndarray\\n        Square distance matrix\\n    plasmid_ids : list\\n        List of plasmid IDs\\n    thresholds : dict\\n        Dictionary of bin -> distance threshold\\n        \\n    Returns:\\n    --------\\n    pd.DataFrame : DataFrame with LIN assignments\\n    \"\"\"\n    # Convert distance matrix to condensed form for linkage\n    condensed_dist = squareform(dist_matrix)\n    \n    # Perform hierarchical clustering\n    linkage_matrix = linkage(condensed_dist, method='single')\n    \n    # Assign clusters at each threshold\n    lin_assignments = pd.DataFrame({'plasmid_id': plasmid_ids})\n    \n    for bin_level, threshold in sorted(thresholds.items()):\n        # Get cluster assignments at this threshold\n        clusters = fcluster(linkage_matrix, threshold, criterion='distance')\n        lin_assignments[f'bin_{bin_level}'] = clusters\n    \n    print(f\"Assigned LIN bins for {len(plasmid_ids)} plasmids\")\n    \n    # Display statistics for each bin\n    for bin_level in sorted(thresholds.keys()):\n        n_clusters = lin_assignments[f'bin_{bin_level}'].nunique()\n        print(f\"Bin {bin_level}: {n_clusters} clusters at {thresholds[bin_level]:.3f} distance (~{(1-thresholds[bin_level])*100:.1f}% ANI)\")\n    \n    return lin_assignments\n\n# Perform LIN binning\nif 'hybrid_distances' in locals() and hybrid_distances is not None:\n    dist_matrix, plasmid_ids = create_distance_matrix_for_clustering(hybrid_distances)\n    lin_assignments = assign_lin_bins(dist_matrix, plasmid_ids, LIN_THRESHOLDS)\n    \n    print(\"\\nSample LIN assignments:\")\n    print(lin_assignments.head(10))\nelse:\n    print(\"Hybrid distances not available. Run previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r67mvyq3jkl",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 4: LIN Code Assignment\n",
    "\n",
    "Create final LIN codes (e.g., `1.2.5.8.0.0`) for each plasmid based on cluster membership at each hierarchical level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "l0zfwftsb0i",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIN assignments not available. Run previous cells first.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create LIN Codes\n\ndef create_lin_codes(lin_assignments: pd.DataFrame, n_bins: int = 6) -> pd.DataFrame:\n    \"\"\"\\n    Create final LIN codes from bin assignments.\\n    \\n    Parameters:\\n    -----------\\n    lin_assignments : pd.DataFrame\\n        DataFrame with bin assignments\\n    n_bins : int\\n        Number of hierarchical bins\\n        \\n    Returns:\\n    --------\\n    pd.DataFrame : DataFrame with LIN codes\\n    \"\"\"\n    result = lin_assignments.copy()\n    \n    # Create LIN code string\n    lin_codes = []\n    for _, row in result.iterrows():\n        code_parts = [str(row[f'bin_{i}']) for i in range(n_bins)]\n        lin_code = '.'.join(code_parts)\n        lin_codes.append(lin_code)\n    \n    result['LIN_code'] = lin_codes\n    \n    print(f\"Created LIN codes for {len(result)} plasmids\")\n    print(f\"Number of unique LIN codes: {result['LIN_code'].nunique()}\")\n    \n    return result\n\ndef assign_new_plasmid_to_lin(new_plasmid_id: str, \n                               existing_lin_assignments: pd.DataFrame,\n                               hybrid_distances: pd.DataFrame) -> str:\n    \"\"\"\\n    Assign a LIN code to a new plasmid based on its closest neighbor.\\n    \\n    This is the key algorithm for real-time plasmid typing.\\n    \\n    Parameters:\\n    -----------\\n    new_plasmid_id : str\\n        ID of new plasmid to assign\\n    existing_lin_assignments : pd.DataFrame\\n        Existing LIN assignments\\n    hybrid_distances : pd.DataFrame\\n        Distance matrix including new plasmid\\n        \\n    Returns:\\n    --------\\n    str : LIN code for new plasmid\\n    \"\"\"\n    # Find distances to all existing plasmids\n    new_distances = hybrid_distances[\n        (hybrid_distances['plasmid1'] == new_plasmid_id) | \n        (hybrid_distances['plasmid2'] == new_plasmid_id)\n    ].copy()\n    \n    # Get closest neighbor\n    if len(new_distances) == 0:\n        return \"0.0.0.0.0.0\"  # Default for isolated plasmid\n    \n    closest_idx = new_distances['hybrid_distance'].idxmin()\n    closest_row = new_distances.loc[closest_idx]\n    \n    # Identify the neighbor\n    neighbor_id = closest_row['plasmid1'] if closest_row['plasmid2'] == new_plasmid_id else closest_row['plasmid2']\n    closest_distance = closest_row['hybrid_distance']\n    \n    # Get neighbor's LIN code\\n    neighbor_lin = existing_lin_assignments[\\n        existing_lin_assignments['plasmid_id'] == neighbor_id\\n    ]['LIN_code'].values[0]\\n    \\n    # Determine at which bin level to assign the same code\\n    # Based on distance thresholds\\n    neighbor_bins = [int(x) for x in neighbor_lin.split('.')]\\n    new_bins = neighbor_bins.copy()\\n    \\n    for bin_level, threshold in sorted(LIN_THRESHOLDS.items()):\\n        if closest_distance > threshold:\\n            # Too distant at this level - assign new cluster\\n            new_bins[bin_level] = max(existing_lin_assignments[f'bin_{bin_level}']) + 1\\n            # All finer bins also get new numbers\\n            for deeper_bin in range(bin_level + 1, len(new_bins)):\\n                new_bins[deeper_bin] = 0\\n            break\\n    \\n    new_lin_code = '.'.join(str(b) for b in new_bins)\\n    \\n    print(f\"Assigned LIN code {new_lin_code} to {new_plasmid_id}\")\\n    print(f\"Closest neighbor: {neighbor_id} (distance: {closest_distance:.4f})\")\\n    \\n    return new_lin_code\\n\\n# Create LIN codes\\nif 'lin_assignments' in locals():\\n    lin_codes_df = create_lin_codes(lin_assignments)\\n    \\n    print(\"\\nSample LIN codes:\")\\n    print(lin_codes_df[['plasmid_id', 'LIN_code']].head(15))\\n    \\n    # Show distribution of codes\\n    print(\"\\nMost common LIN codes:\")\\n    print(lin_codes_df['LIN_code'].value_counts().head(10))\\nelse:\\n    print(\"LIN assignments not available. Run previous cells first.\")\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3qqptcdhscn",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Innovation: Backbone-First Indexing (Dual-LIN)\n",
    "\n",
    "Plasmids consist of:\n",
    "- **Stable backbone**: Replication/transfer genes\n",
    "- **Variable accessory region**: Antibiotic resistance genes, integrons, transposons\n",
    "\n",
    "**Approach**: Create a Partitioned LIN where:\n",
    "- First 3 bins (0-2) represent the stable backbone\n",
    "- Last 3 bins (3-5) represent mobile accessory elements\n",
    "\n",
    "This allows tracking plasmid outbreaks even when resistance genes are acquired/lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "xjynd21zibh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No plasmid data available.\n"
     ]
    }
   ],
   "source": [
    "# Backbone-First Indexing Implementation\n\ndef identify_plasmid_backbone_regions(plasmid_sequence: str) -> Tuple[str, str]:\n    \"\"\"\\n    Identify backbone vs accessory regions in a plasmid.\\n    \\n    This is a simplified version. In practice, use:\\n    - MOB-suite for conjugation/mobilization regions\\n    - Prokka or PGAP for gene annotation\\n    - AMRFinderPlus for resistance genes\\n    - IntegronFinder for integrons\\n    \\n    Parameters:\\n    -----------\\n    plasmid_sequence : str\\n        Plasmid DNA sequence\\n        \\n    Returns:\\n    --------\\n    tuple : (backbone_sequence, accessory_sequence)\\n    \"\"\"\n    # Placeholder implementation\n    # In real application, annotate genes and separate by function\n    \n    # Simple approximation: first 60% = backbone, last 40% = accessory\n    backbone_length = int(len(plasmid_sequence) * 0.6)\n    \n    backbone = plasmid_sequence[:backbone_length]\n    accessory = plasmid_sequence[backbone_length:]\n    \n    return backbone, accessory\n\ndef create_dual_lin_codes(plasmid_data: Dict, \n                          backbone_thresholds: Dict[int, float],\n                          accessory_thresholds: Dict[int, float]) -> pd.DataFrame:\n    \"\"\"\\n    Create Dual-LIN codes with separate bins for backbone and accessory regions.\\n    \\n    Parameters:\\n    -----------\\n    plasmid_data : dict\\n        Dictionary with plasmid sequences\\n    backbone_thresholds : dict\\n        Thresholds for backbone bins (0-2)\\n    accessory_thresholds : dict\\n        Thresholds for accessory bins (3-5)\\n        \\n    Returns:\\n    --------\\n    pd.DataFrame : DataFrame with Dual-LIN codes\\n    \"\"\"\n    # Separate backbone and accessory sequences\n    backbone_sequences = {}\n    accessory_sequences = {}\n    \n    for plasmid_id, data in plasmid_data.items():\n        backbone, accessory = identify_plasmid_backbone_regions(data['sequence'])\n        backbone_sequences[plasmid_id] = backbone\n        accessory_sequences[plasmid_id] = accessory\n    \n    print(f\"Separated {len(backbone_sequences)} plasmids into backbone and accessory regions\")\n    \n    # TODO: Calculate separate distance matrices for backbone and accessory\n    # This would require running Mash twice (once for each region)\n    \n    # For now, return a placeholder\n    print(\"\\nNOTE: Full Dual-LIN implementation requires:\")\n    print(\"1. Gene annotation (Prokka/PGAP)\")\n    print(\"2. Functional classification (backbone vs accessory)\")\n    print(\"3. Separate distance calculations for each region\")\n    print(\"4. Combined LIN code: backbone_bins.accessory_bins\")\n    \n    return pd.DataFrame({\n        'plasmid_id': list(plasmid_data.keys()),\n        'backbone_length': [len(backbone_sequences[pid]) for pid in plasmid_data.keys()],\n        'accessory_length': [len(accessory_sequences[pid]) for pid in plasmid_data.keys()]\n    })\n\n# Define separate thresholds for backbone and accessory regions\nBACKBONE_THRESHOLDS = {\n    0: 0.15,   # Backbone Family\n    1: 0.10,   # Backbone Subfamily  \n    2: 0.05    # Backbone Cluster\n}\n\nACCESSORY_THRESHOLDS = {\n    3: 0.10,   # Accessory Cluster\n    4: 0.05,   # Accessory Subcluster\n    5: 0.01    # Accessory Clone\n}\n\n# Create Dual-LIN analysis\nif len(plasmid_data) > 0:\n    dual_lin_analysis = create_dual_lin_codes(\n        plasmid_data, \n        BACKBONE_THRESHOLDS, \n        ACCESSORY_THRESHOLDS\n    )\n    \n    print(\"\\nBackbone/Accessory region statistics:\")\n    print(dual_lin_analysis.describe())\nelse:\n    print(\"No plasmid data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k78skfsqe7f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Innovation: Handling Mosaicism (The \"Pling\" Approach)\n",
    "\n",
    "Plasmids are often mosaic - pieces of different plasmids stitched together through recombination.\n",
    "\n",
    "**Approach**:\n",
    "- Use k-mer based sketching (Mash/Sourmash) for initial bins (coarse resolution)\n",
    "- Switch to graph-based distances for final bins (high resolution)\n",
    "- Helps identify hybrid/chimeric plasmids in outbreak investigations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "oq3lbzenfdq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No plasmid data available.\n"
     ]
    }
   ],
   "source": [
    "# Mosaicism Detection using k-mer Analysis\n\ndef detect_mosaic_plasmids(plasmid_data: Dict, sketch_size: int = 1000) -> pd.DataFrame:\n    \"\"\"\\n    Detect potential mosaic/chimeric plasmids using k-mer composition analysis.\\n    \\n    Strategy:\\n    1. Create k-mer sketches for all plasmids\\n    2. Identify plasmids with unusual k-mer sharing patterns\\n    3. Flag potential mosaics for further investigation\\n    \\n    Parameters:\\n    -----------\\n    plasmid_data : dict\\n        Dictionary with plasmid sequences\\n    sketch_size : int\\n        Size of Mash sketch\\n        \\n    Returns:\\n    --------\\n    pd.DataFrame : Mosaic detection results\\n    \"\"\"\n    print(\"Mosaic Detection Analysis\")\n    print(\"=\" * 50)\n    \n    # Placeholder for mosaic detection\n    # Real implementation would use:\n    # 1. Mash/Sourmash for k-mer sketching\n    # 2. Graph-based analysis (e.g., assembly graphs)\n    # 3. Breakpoint detection algorithms\n    \n    results = []\n    \n    for plasmid_id, data in plasmid_data.items():\n        # Simple heuristic: check for unusual GC content variation\n        # Real implementation would be much more sophisticated\n        \n        seq = data['sequence']\n        \n        if len(seq) == 0:\n            continue\n            \n        # Calculate GC content in sliding windows\n        window_size = max(1000, len(seq) // 10)\n        gc_contents = []\n        \n        for i in range(0, len(seq) - window_size, window_size // 2):\n            window = seq[i:i+window_size]\n            gc = (window.count('G') + window.count('C')) / len(window)\n            gc_contents.append(gc)\n        \n        if len(gc_contents) > 1:\n            gc_std = np.std(gc_contents)\n            gc_mean = np.mean(gc_contents)\n            \n            # Flag if GC content varies significantly\n            is_mosaic_candidate = gc_std > 0.05\n            \n            results.append({\n                'plasmid_id': plasmid_id,\n                'gc_mean': gc_mean,\n                'gc_std': gc_std,\n                'mosaic_candidate': is_mosaic_candidate,\n                'confidence': 'low' if gc_std < 0.1 else 'medium' if gc_std < 0.15 else 'high'\n            })\n    \n    mosaic_df = pd.DataFrame(results)\n    \n    if len(mosaic_df) > 0:\n        n_candidates = mosaic_df['mosaic_candidate'].sum()\n        print(f\"\\nAnalyzed {len(mosaic_df)} plasmids\")\n        print(f\"Mosaic candidates: {n_candidates} ({n_candidates/len(mosaic_df)*100:.1f}%)\")\n        print(f\"\\nGC content statistics:\")\n        print(mosaic_df[['gc_mean', 'gc_std']].describe())\n    \n    return mosaic_df\n\ndef create_sourmash_signatures(plasmid_data: Dict, output_dir: str = \"./output/sourmash\") -> List[str]:\n    \"\"\"\\n    Create Sourmash signatures for advanced k-mer analysis.\\n    \\n    Sourmash allows for more sophisticated analysis including:\\n    - Multiple k-mer sizes\\n    - MinHash sketching\\n    - Genome comparison at different resolutions\\n    \\n    Parameters:\\n    -----------\\n    plasmid_data : dict\\n        Dictionary with plasmid sequences\\n    output_dir : str\\n        Directory for signatures\\n        \\n    Returns:\\n    --------\\n    list : Paths to signature files\\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    print(\"\\nCreating Sourmash signatures...\")\n    print(\"NOTE: This requires Sourmash to be installed:\")\n    print(\"  conda install -c bioconda sourmash\")\n    \n    # Write sequences to FASTA\n    fasta_file = os.path.join(output_dir, \"plasmids.fasta\")\n    with open(fasta_file, 'w') as f:\n        for plasmid_id, data in plasmid_data.items():\n            f.write(f\">{plasmid_id}\\\\n{data['sequence']}\\\\n\")\n    \n    # Create signatures with multiple k-mer sizes\n    sig_files = []\n    for k in [21, 31, 51]:\n        sig_file = os.path.join(output_dir, f\"plasmids_k{k}.sig\")\n        cmd = f\"sourmash sketch dna -p k={k},scaled=1000 {fasta_file} -o {sig_file}\"\n        \n        try:\n            print(f\"  Creating k={k} signature...\")\n            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n            if result.returncode == 0:\n                sig_files.append(sig_file)\n                print(f\"    \u2713 Created {sig_file}\")\n            else:\n                print(f\"    \u2717 Failed: {result.stderr}\")\n        except Exception as e:\n            print(f\"    \u2717 Error: {e}\")\n    \n    return sig_files\n\n# Detect mosaic plasmids\nif len(plasmid_data) > 0:\n    mosaic_analysis = detect_mosaic_plasmids(plasmid_data)\n    \n    if len(mosaic_analysis) > 0 and mosaic_analysis['mosaic_candidate'].sum() > 0:\n        print(\"\\nPotential mosaic plasmids:\")\n        print(mosaic_analysis[mosaic_analysis['mosaic_candidate']][['plasmid_id', 'gc_mean', 'gc_std', 'confidence']])\n    \n    # Optionally create Sourmash signatures for advanced analysis\n    # Uncomment if Sourmash is installed:\n    # sourmash_sigs = create_sourmash_signatures(plasmid_data)\nelse:\n    print(\"No plasmid data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s7t520kmldn",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualization and Outbreak Tracking\n",
    "\n",
    "Visualize LIN codes and track plasmid transmission across bacterial species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13lgkghhpse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid distances not available for visualization.\n"
     ]
    }
   ],
   "source": [
    "# Visualization: Distance Matrix Heatmap\n",
    "\n",
    "def plot_distance_heatmap(hybrid_distances: pd.DataFrame, max_plasmids: int = 50):\n",
    "    \"\"\"\n",
    "    Create a heatmap of pairwise distances.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    hybrid_distances : pd.DataFrame\n",
    "        DataFrame with pairwise distances\n",
    "    max_plasmids : int\n",
    "        Maximum number of plasmids to display\n",
    "    \"\"\"\n",
    "    if hybrid_distances is None or len(hybrid_distances) == 0:\n",
    "        print(\"No distance data available\")\n",
    "        return\n",
    "    \n",
    "    # Get unique plasmids\n",
    "    plasmids = sorted(set(list(hybrid_distances['plasmid1']) + list(hybrid_distances['plasmid2'])))\n",
    "    \n",
    "    if len(plasmids) > max_plasmids:\n",
    "        print(f\"Limiting display to first {max_plasmids} plasmids\")\n",
    "        plasmids = plasmids[:max_plasmids]\n",
    "    \n",
    "    # Create distance matrix\n",
    "    n = len(plasmids)\n",
    "    dist_matrix = np.zeros((n, n))\n",
    "    id_to_idx = {pid: idx for idx, pid in enumerate(plasmids)}\n",
    "    \n",
    "    for _, row in hybrid_distances.iterrows():\n",
    "        if row['plasmid1'] in id_to_idx and row['plasmid2'] in id_to_idx:\n",
    "            i = id_to_idx[row['plasmid1']]\n",
    "            j = id_to_idx[row['plasmid2']]\n",
    "            dist_matrix[i, j] = row['hybrid_distance']\n",
    "            dist_matrix[j, i] = row['hybrid_distance']\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Shorten plasmid IDs for display\n",
    "    short_ids = [os.path.basename(p)[:20] for p in plasmids]\n",
    "    \n",
    "    sns.heatmap(dist_matrix, \n",
    "                xticklabels=short_ids,\n",
    "                yticklabels=short_ids,\n",
    "                cmap='viridis_r',\n",
    "                cbar_kws={'label': 'Hybrid Distance'},\n",
    "                square=True)\n",
    "    \n",
    "    plt.title('Plasmid Pairwise Distance Matrix\\\n(Lower distance = more similar)', \n",
    "              fontsize=14, pad=20)\n",
    "    plt.xlabel('Plasmid ID', fontsize=12)\n",
    "    plt.ylabel('Plasmid ID', fontsize=12)\n",
    "    plt.xticks(rotation=90, fontsize=8)\n",
    "    plt.yticks(rotation=0, fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Displayed {n}x{n} distance matrix\")\n",
    "\n",
    "# Plot distance heatmap\n",
    "if 'hybrid_distances' in locals() and hybrid_distances is not None:\n",
    "    plot_distance_heatmap(hybrid_distances)\n",
    "else:\n",
    "    print(\"Hybrid distances not available for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9p5b5wikfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIN codes not available for visualization.\n"
     ]
    }
   ],
   "source": [
    "# Visualization: LIN Code Distribution\n",
    "\n",
    "def plot_lin_distribution(lin_codes_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Visualize distribution of LIN codes at each hierarchical level.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lin_codes_df : pd.DataFrame\n",
    "        DataFrame with LIN codes\n",
    "    \"\"\"\n",
    "    if lin_codes_df is None or len(lin_codes_df) == 0:\n",
    "        print(\"No LIN code data available\")\n",
    "        return\n",
    "    \n",
    "    # Count unique codes at each bin level\n",
    "    bin_levels = [col for col in lin_codes_df.columns if col.startswith('bin_')]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, bin_col in enumerate(bin_levels[:6]):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Count cluster sizes\n",
    "        cluster_counts = lin_codes_df[bin_col].value_counts()\n",
    "        \n",
    "        # Plot distribution\n",
    "        ax.hist(cluster_counts.values, bins=min(30, len(cluster_counts)), \n",
    "                edgecolor='black', alpha=0.7)\n",
    "        \n",
    "        bin_level = int(bin_col.split('_')[1])\n",
    "        threshold = LIN_THRESHOLDS[bin_level]\n",
    "        ani_pct = (1 - threshold) * 100\n",
    "        \n",
    "        ax.set_xlabel('Cluster Size (# plasmids)', fontsize=10)\n",
    "        ax.set_ylabel('Frequency', fontsize=10)\n",
    "        ax.set_title(f'Bin {bin_level}: ~{ani_pct:.1f}% ANI\\\n{len(cluster_counts)} clusters', \n",
    "                     fontsize=11)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('LIN Code Distribution Across Hierarchical Levels', \n",
    "                 fontsize=14, fontweight='bold', y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\\nCluster statistics by bin level:\")\n",
    "    for bin_col in bin_levels[:6]:\n",
    "        cluster_counts = lin_codes_df[bin_col].value_counts()\n",
    "        print(f\"{bin_col}: {len(cluster_counts)} clusters, \"\n",
    "              f\"avg size={cluster_counts.mean():.1f}, \"\n",
    "              f\"max size={cluster_counts.max()}\")\n",
    "\n",
    "# Plot LIN distribution\n",
    "if 'lin_codes_df' in locals():\n",
    "    plot_lin_distribution(lin_codes_df)\n",
    "else:\n",
    "    print(\"LIN codes not available for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e96k71jgiw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIN codes not available for outbreak simulation.\n"
     ]
    }
   ],
   "source": [
    "# Outbreak Tracking: Horizontal Gene Transfer Detection\n",
    "\n",
    "def simulate_outbreak_scenario(lin_codes_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simulate an outbreak scenario showing plasmid transmission across bacterial species.\n",
    "    \n",
    "    In a real outbreak:\n",
    "    - Same LIN code in different species = evidence of horizontal gene transfer (HGT)\n",
    "    - Standard chromosome typing would miss this connection\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    lin_codes_df : pd.DataFrame\n",
    "        DataFrame with LIN codes\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Simulated outbreak data\n",
    "    \"\"\"\n",
    "    if lin_codes_df is None or len(lin_codes_df) == 0:\n",
    "        print(\"No LIN codes available\")\n",
    "        return None\n",
    "    \n",
    "    # Simulate bacterial species assignments\n",
    "    species_options = [\n",
    "        'Escherichia coli',\n",
    "        'Klebsiella pneumoniae',\n",
    "        'Salmonella enterica',\n",
    "        'Enterobacter cloacae',\n",
    "        'Citrobacter freundii'\n",
    "    ]\n",
    "    \n",
    "    outbreak_data = lin_codes_df.copy()\n",
    "    \n",
    "    # Randomly assign species\n",
    "    np.random.seed(42)\n",
    "    outbreak_data['bacterial_species'] = np.random.choice(\n",
    "        species_options, \n",
    "        size=len(outbreak_data)\n",
    "    )\n",
    "    \n",
    "    # Simulate isolation dates\n",
    "    base_date = pd.Timestamp('2025-01-01')\n",
    "    outbreak_data['isolation_date'] = [\n",
    "        base_date + pd.Timedelta(days=int(np.random.exponential(30)))\n",
    "        for _ in range(len(outbreak_data))\n",
    "    ]\n",
    "    \n",
    "    # Simulate patient IDs\n",
    "    outbreak_data['patient_id'] = [\n",
    "        f\"PT{str(i).zfill(4)}\" \n",
    "        for i in np.random.randint(1, 100, size=len(outbreak_data))\n",
    "    ]\n",
    "    \n",
    "    return outbreak_data\n",
    "\n",
    "def detect_horizontal_transfer(outbreak_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detect evidence of horizontal gene transfer based on LIN codes.\n",
    "    \n",
    "    HGT Evidence:\n",
    "    - Same LIN code found in multiple bacterial species\n",
    "    - Same LIN code found in different patients\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    outbreak_data : pd.DataFrame\n",
    "        Outbreak data with LIN codes and metadata\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : HGT detection results\n",
    "    \"\"\"\n",
    "    if outbreak_data is None or 'LIN_code' not in outbreak_data.columns:\n",
    "        print(\"Invalid outbreak data\")\n",
    "        return None\n",
    "    \n",
    "    # Group by LIN code\n",
    "    hgt_evidence = []\n",
    "    \n",
    "    for lin_code, group in outbreak_data.groupby('LIN_code'):\n",
    "        if len(group) < 2:\n",
    "            continue\n",
    "            \n",
    "        n_species = group['bacterial_species'].nunique()\n",
    "        n_patients = group['patient_id'].nunique()\n",
    "        \n",
    "        if n_species > 1:\n",
    "            hgt_evidence.append({\n",
    "                'LIN_code': lin_code,\n",
    "                'n_isolates': len(group),\n",
    "                'n_species': n_species,\n",
    "                'species_list': ', '.join(group['bacterial_species'].unique()),\n",
    "                'n_patients': n_patients,\n",
    "                'date_range': f\"{group['isolation_date'].min().date()} to {group['isolation_date'].max().date()}\",\n",
    "                'hgt_confidence': 'HIGH' if n_species >= 3 else 'MEDIUM'\n",
    "            })\n",
    "    \n",
    "    hgt_df = pd.DataFrame(hgt_evidence)\n",
    "    \n",
    "    if len(hgt_df) > 0:\n",
    "        hgt_df = hgt_df.sort_values('n_species', ascending=False)\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"HORIZONTAL GENE TRANSFER DETECTION\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\\nDetected {len(hgt_df)} LIN codes with evidence of HGT\")\n",
    "        print(f\"Total isolates involved: {hgt_df['n_isolates'].sum()}\")\n",
    "        print(f\"\\\nTop HGT events (plasmid found in multiple species):\\\n\")\n",
    "        print(hgt_df.to_string(index=False))\n",
    "        \n",
    "        print(\"\\\n\" + \"=\" * 80)\n",
    "        print(\"INTERPRETATION:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Finding the same plasmid LIN code in different bacterial species is\")\n",
    "        print(\"definitive proof of horizontal gene transfer (HGT).\")\n",
    "        print(\"\\\nStandard chromosome-based typing would MISS this connection entirely!\")\n",
    "        print(\"=\" * 80)\n",
    "    else:\n",
    "        print(\"No evidence of HGT detected in this dataset\")\n",
    "    \n",
    "    return hgt_df\n",
    "\n",
    "# Simulate outbreak and detect HGT\n",
    "if 'lin_codes_df' in locals():\n",
    "    outbreak_data = simulate_outbreak_scenario(lin_codes_df)\n",
    "    \n",
    "    if outbreak_data is not None:\n",
    "        print(\"\\\nSimulated outbreak data:\")\n",
    "        print(outbreak_data[['plasmid_id', 'LIN_code', 'bacterial_species', 'patient_id', 'isolation_date']].head(10))\n",
    "        \n",
    "        hgt_results = detect_horizontal_transfer(outbreak_data)\n",
    "else:\n",
    "    print(\"LIN codes not available for outbreak simulation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8o9od8dcid",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Built\n",
    "\n",
    "This notebook implements a novel **Plasmid Life Identification Number (pLIN)** system with three key innovations:\n",
    "\n",
    "1. **Hybrid Metric (ANI + Coverage)**: Length-adjusted distance metric that prevents false relationships based on single shared genes\n",
    "2. **Backbone-First Indexing**: Separates stable backbone from variable accessory regions for robust outbreak tracking\n",
    "3. **Mosaicism Handling**: k-mer sketching approach to identify chimeric/hybrid plasmids\n",
    "\n",
    "### Workflow Summary\n",
    "\n",
    "| Step | Description | Implementation |\n",
    "|------|-------------|----------------|\n",
    "| 1. Dataset Collection | Load plasmid sequences | BioPython FASTA parsing |\n",
    "| 2. Distance Matrix | Calculate pairwise distances | Mash (k-mer) + Hybrid metric |\n",
    "| 3. LIN Binning | Hierarchical clustering | Single-linkage at 6 thresholds |\n",
    "| 4. LIN Assignment | Create stable codes | Closest-neighbor algorithm |\n",
    "\n",
    "### Why This Matters for Outbreak Tracking\n",
    "\n",
    "When you see the same plasmid LIN code (e.g., `1.2.5.8.0.0`) in:\n",
    "- **Different bacterial species** (E. coli + Klebsiella)\n",
    "- **Different patients**\n",
    "- **Different time points**\n",
    "\n",
    "You have **definitive proof of horizontal gene transfer (HGT)** that standard chromosome-based typing would completely miss.\n",
    "\n",
    "### Next Steps for Implementation\n",
    "\n",
    "1. **Real Distance Calculations**:\n",
    "   - Install and run Mash/FastANI on real plasmid datasets\n",
    "   - Implement actual alignment-based coverage calculation\n",
    "   - Replace simulated data with PLSDB or NCBI downloads\n",
    "\n",
    "2. **Backbone/Accessory Separation**:\n",
    "   - Use MOB-suite for conjugation region identification\n",
    "   - Annotate genes with Prokka/PGAP\n",
    "   - Classify genes as backbone vs accessory\n",
    "\n",
    "3. **Threshold Optimization**:\n",
    "   - Analyze your specific plasmid family\n",
    "   - Adjust ANI thresholds based on biological diversity\n",
    "   - Validate against known outbreak data\n",
    "\n",
    "4. **Database Integration**:\n",
    "   - Build a LINbase-style database\n",
    "   - Implement real-time LIN assignment API\n",
    "   - Create web interface for plasmid typing\n",
    "\n",
    "### Tools to Install\n",
    "\n",
    "```bash\n",
    "# Install via conda\n",
    "conda install -c bioconda mash fastani sourmash\n",
    "conda install -c bioconda mob-suite prokka\n",
    "conda install -c bioconda amrfinderplus\n",
    "```\n",
    "\n",
    "### References\n",
    "\n",
    "- Rodr\u00edguez-P\u00e9rez et al. (2020) \"Life Identification Number (LIN) for bacterial typing\" [DOI: 10.1038/s41598-020-77696-0]\n",
    "- Ondov et al. (2016) \"Mash: fast genome and metagenome distance estimation using MinHash\" [DOI: 10.1186/s13059-016-0997-x]\n",
    "- Robertson & Nash (2018) \"MOB-suite: software tools for clustering, reconstruction and typing of plasmids\" [DOI: 10.1099/mgen.0.000206]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13sp06pzxhj",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Machine Learning: Nested Cross-Validation Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This section implements **nested cross-validation (CV)** for plasmid classification tasks:\n",
    "\n",
    "### Prediction Tasks\n",
    "1. **Plasmid Family Classification**: Predict plasmid incompatibility group (Inc type)\n",
    "2. **Host Range Prediction**: Predict bacterial host range (narrow vs broad)\n",
    "3. **Resistance Profile Prediction**: Predict antibiotic resistance patterns\n",
    "4. **LIN Code Prediction**: Predict high-level LIN bins from sequence features\n",
    "\n",
    "### Why Nested CV?\n",
    "\n",
    "**Nested CV** prevents data leakage and provides unbiased performance estimates:\n",
    "- **Outer loop**: Performance estimation (K-fold CV)\n",
    "- **Inner loop**: Model selection and hyperparameter tuning\n",
    "- Ensures test data is never used for model selection\n",
    "\n",
    "### ML Pipeline\n",
    "1. Feature engineering from plasmid sequences\n",
    "2. Nested CV setup with stratification\n",
    "3. Multiple model comparison (RF, XGBoost, SVM)\n",
    "4. Hyperparameter optimization with Optuna\n",
    "5. Model evaluation and interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "o3z0avsr3p",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning environment ready!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sklearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMachine Learning environment ready!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mScikit-learn version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43msklearn\u001b[49m.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mXGBoost version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxgb.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOptuna version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptuna.__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "# ML Imports\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    roc_curve\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    cross_validate\n",
    ")\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import OptunaSearchCV\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import HyperbandPruner\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Machine Learning environment ready!\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cx6szc7ahh",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Feature Engineering\n",
    "\n",
    "Extract meaningful features from plasmid sequences for ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40sf41g1254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for Plasmid Classification\n# Load ALL three Inc types for multi-class ML classification\n\nfrom itertools import product as iter_product\n\ndef extract_plasmid_features_fast(plasmid_data: Dict) -> pd.DataFrame:\n    \"\"\"Extract features from plasmid sequences using optimized numpy approach.\"\"\"\n    bases = 'ACGT'\n    base_idx = {b: i for i, b in enumerate(bases)}\n    \n    # Precompute all dinucleotides and trinucleotides\n    dinucs = [''.join(p) for p in iter_product(bases, repeat=2)]\n    trinucs_selected = ['ATG','TAA','TAG','TGA','GCG','CGC','AAA','TTT','CCC','GGG']\n    \n    features_list = []\n    total = len(plasmid_data)\n    \n    for idx, (plasmid_id, data) in enumerate(plasmid_data.items()):\n        seq = data['sequence'].upper()\n        length = len(seq)\n        \n        if length == 0:\n            continue\n        \n        a = seq.count('A')\n        t = seq.count('T')\n        g = seq.count('G')\n        c = seq.count('C')\n        \n        gc = (g + c) / length\n        at = (a + t) / length\n        at_skew = (a - t) / (a + t) if (a + t) > 0 else 0\n        gc_skew = (g - c) / (g + c) if (g + c) > 0 else 0\n        \n        features = {\n            'plasmid_id': plasmid_id,\n            'length': length,\n            'gc_content': gc,\n            'at_content': at,\n            'at_skew': at_skew,\n            'gc_skew': gc_skew,\n            'log_length': np.log10(length) if length > 0 else 0,\n        }\n        \n        # Dinucleotide frequencies - count using string.count() which is C-optimized\n        total_di = max(length - 1, 1)\n        for di in dinucs:\n            features[f'di_{di}'] = seq.count(di) / total_di\n        \n        # Selected trinucleotide frequencies\n        total_tri = max(length - 2, 1)\n        for tri in trinucs_selected:\n            features[f'tri_{tri}'] = seq.count(tri) / total_tri\n        \n        features_list.append(features)\n        \n        if (idx + 1) % 1000 == 0:\n            print(f\"  Processed {idx+1}/{total} plasmids\")\n    \n    return pd.DataFrame(features_list)\n\n# Load ALL Inc types for multi-class classification\nimport glob as glob_module\n\ninc_types = {\n    'IncFII': './plasmid_sequences_for_training/IncFII/fastas',\n    'IncN': './plasmid_sequences_for_training/IncN/fastas',\n    'IncX1': './plasmid_sequences_for_training/IncX1/fastas',\n}\n\nall_plasmid_data = {}\nlabels = {}\n\nfor inc_type, fasta_dir in inc_types.items():\n    fasta_files = glob_module.glob(os.path.join(fasta_dir, \"*.fasta\"))\n    count = 0\n    for fasta_file in fasta_files:\n        try:\n            for record in SeqIO.parse(fasta_file, \"fasta\"):\n                pid = f\"{inc_type}__{record.id}\"\n                all_plasmid_data[pid] = {\n                    'id': record.id,\n                    'sequence': str(record.seq),\n                    'length': len(record.seq),\n                }\n                labels[pid] = inc_type\n                count += 1\n        except Exception:\n            pass\n    print(f\"Loaded {count} sequences from {inc_type}\")\n\nprint(f\"\\nTotal plasmids loaded: {len(all_plasmid_data)}\")\n\n# Extract features using fast method\nprint(\"\\nExtracting features...\")\nplasmid_features = extract_plasmid_features_fast(all_plasmid_data)\nprint(f\"Extracted {len(plasmid_features.columns)-1} features for {len(plasmid_features)} plasmids\")\n\nprint(f\"\\nFeature categories:\")\nprint(f\"  - Basic composition: 7 features\")\nprint(f\"  - Dinucleotide frequencies: 16 features\")\nprint(f\"  - 3-mer frequencies: 10 features\")\n\nprint(f\"\\nFeature matrix shape: {plasmid_features.shape}\")\nprint(f\"\\nFirst few features:\")\nprint(plasmid_features.head())\n\n# Create target column\n# Subsample for faster ML execution\nimport random\nrandom.seed(42)\nmax_samples_per_class = 500\nsampled_ids = []\nfor inc_type in set(labels.values()):\n    ids_for_type = [k for k, v in labels.items() if v == inc_type]\n    if len(ids_for_type) > max_samples_per_class:\n        sampled_ids.extend(random.sample(ids_for_type, max_samples_per_class))\n    else:\n        sampled_ids.extend(ids_for_type)\nlabels_subset = {k: labels[k] for k in sampled_ids}\nprint(f\"Subsampled to {len(labels_subset)} plasmids for ML\")\nlabels_df = pd.DataFrame([{'plasmid_id': k, 'target': v} for k, v in labels_subset.items()])\nml_data = plasmid_features.merge(labels_df, on='plasmid_id')\nprint(f\"\\nML dataset ready: {ml_data.shape}\")\nprint(f\"Class distribution:\")\nprint(ml_data['target'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sb180gygz0s",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Nested Cross-Validation Setup\n",
    "\n",
    "Implement rigorous nested CV to prevent overfitting and data leakage.\n",
    "\n",
    "**Structure**:\n",
    "- **Outer CV (5-fold)**: Unbiased performance estimation\n",
    "- **Inner CV (3-fold)**: Hyperparameter tuning and model selection\n",
    "- **Stratified splits**: Maintain class distribution in each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frejhawnvzs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested Cross-Validation Framework\n\nclass NestedCVFramework:\n    \"\"\"\\n    Nested Cross-Validation framework for unbiased model evaluation.\\n    \\n    Prevents data leakage by:\\n    1. Splitting data into outer folds (test sets)\\n    2. For each outer fold, using inner CV for hyperparameter tuning\\n    3. Never using test data for model selection\\n    \"\"\"\n    \n    def __init__(self, n_outer_folds: int = 3, n_inner_folds: int = 2, random_state: int = 42):\n        \"\"\"\\n        Initialize nested CV framework.\\n        \\n        Parameters:\\n        -----------\\n        n_outer_folds : int\\n            Number of outer CV folds (performance estimation)\\n        n_inner_folds : int\\n            Number of inner CV folds (hyperparameter tuning)\\n        random_state : int\\n            Random seed for reproducibility\\n        \"\"\"\n        self.n_outer_folds = n_outer_folds\n        self.n_inner_folds = n_inner_folds\n        self.random_state = random_state\n        \n        # Create stratified CV splitters\n        self.outer_cv = StratifiedKFold(\n            n_splits=n_outer_folds, \n            shuffle=True, \n            random_state=random_state\n        )\n        \n        self.inner_cv = StratifiedKFold(\n            n_splits=n_inner_folds,\n            shuffle=True,\n            random_state=random_state\n        )\n        \n        # Storage for results\n        self.outer_scores = []\n        self.best_params_per_fold = []\n        self.trained_models = []\n        \n        print(f\"Nested CV Framework initialized:\")\n        print(f\"  - Outer CV: {n_outer_folds}-fold (performance estimation)\")\n        print(f\"  - Inner CV: {n_inner_folds}-fold (hyperparameter tuning)\")\n        print(f\"  - Random state: {random_state}\")\n    \n    def get_cv_splits(self) -> Tuple:\n        \"\"\"Return CV splitters.\"\"\"\n        return self.outer_cv, self.inner_cv\n    \n    def prepare_data(self, ml_data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n        \"\"\"\\n        Prepare data for ML training.\\n        \\n        Parameters:\\n        -----------\\n        ml_data : pd.DataFrame\\n            DataFrame with features and target\\n            \\n        Returns:\\n        --------\\n        tuple : (X, y, feature_names)\\n        \"\"\"\n        # Separate features and target\n        feature_cols = [col for col in ml_data.columns \n                       if col not in ['plasmid_id', 'target']]\n        \n        X = ml_data[feature_cols].values\n        y = ml_data['target'].values\n        \n        # Encode target if needed\n        if y.dtype == 'object' or isinstance(y[0], str):\n            le = LabelEncoder()\n            y = le.fit_transform(y)\n            self.label_encoder = le\n        \n        print(f\"Data prepared:\")\n        print(f\"  - Features: {X.shape}\")\n        print(f\"  - Target: {y.shape}\")\n        print(f\"  - Feature names: {len(feature_cols)}\")\n        print(f\"  - Classes: {np.unique(y)}\")\n        \n        return X, y, feature_cols\n\ndef setup_nested_cv(ml_data: pd.DataFrame, \n                    n_outer: int = 5, \n                    n_inner: int = 3) -> Tuple[NestedCVFramework, np.ndarray, np.ndarray]:\n    \"\"\"\\n    Setup nested CV framework and prepare data.\\n    \\n    Parameters:\\n    -----------\\n    ml_data : pd.DataFrame\\n        ML dataset with features and target\\n    n_outer : int\\n        Number of outer folds\\n    n_inner : int\\n        Number of inner folds\\n        \\n    Returns:\\n    --------\\n    tuple : (nested_cv_framework, X, y)\\n    \"\"\"\n    # Initialize framework\n    nested_cv = NestedCVFramework(\n        n_outer_folds=n_outer,\n        n_inner_folds=n_inner,\n        random_state=42\n    )\n    \n    # Prepare data\n    X, y, feature_names = nested_cv.prepare_data(ml_data)\n    \n    # Store feature names for later use\n    nested_cv.feature_names = feature_names\n    \n    print(f\"\\nNested CV ready for {len(np.unique(y))}-class classification\")\n    print(f\"Class distribution: {np.bincount(y)}\")\n    \n    return nested_cv, X, y\n\n# Setup nested CV\nif 'ml_data' in locals() and len(ml_data) > 0:\n    # Filter to ensure we have enough samples per class\n    min_samples_per_class = 5\n    class_counts = ml_data['target'].value_counts()\n    valid_classes = class_counts[class_counts >= min_samples_per_class].index\n    \n    if len(valid_classes) < 2:\n        print(f\"WARNING: Not enough classes with {min_samples_per_class}+ samples\")\n        print(\"Need more data for meaningful nested CV analysis\")\n    else:\n        ml_data_filtered = ml_data[ml_data['target'].isin(valid_classes)].copy()\n        print(f\"Filtered to {len(valid_classes)} classes with sufficient samples\")\n        print(f\"Dataset size: {len(ml_data_filtered)} plasmids\")\n        \n        # Setup nested CV\n        nested_cv, X, y = setup_nested_cv(ml_data_filtered, n_outer=3, n_inner=2)\n        \n        print(\"\\nReady for model training!\")\nelse:\n    print(\"ML data not available. Run previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12zmxs0gl7w",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Model Selection with Hyperparameter Optimization\n",
    "\n",
    "Use Optuna for efficient hyperparameter tuning with Tree-structured Parzen Estimator (TPE).\n",
    "\n",
    "**Models tested**:\n",
    "1. Random Forest\n",
    "2. XGBoost\n",
    "3. Gradient Boosting\n",
    "4. Logistic Regression (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hno88ww8lgm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Search Spaces with Optuna\n",
    "\n",
    "def get_search_space(model_name: str, trial: optuna.Trial):\n",
    "    \"\"\"\n",
    "    Define hyperparameter search spaces for different models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str\n",
    "        Name of the model\n",
    "    trial : optuna.Trial\n",
    "        Optuna trial object\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Hyperparameter search space\n",
    "    \"\"\"\n",
    "    if model_name == 'RandomForest':\n",
    "        return {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "            'class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),\n",
    "            'random_state': 42\n",
    "        }\n",
    "    \n",
    "    elif model_name == 'XGBoost':\n",
    "        return {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "            'random_state': 42,\n",
    "            'tree_method': 'hist',\n",
    "            'eval_metric': 'mlogloss'\n",
    "        }\n",
    "    \n",
    "    elif model_name == 'GradientBoosting':\n",
    "        return {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "            'random_state': 42\n",
    "        }\n",
    "    \n",
    "    elif model_name == 'LogisticRegression':\n",
    "        return {\n",
    "            'C': trial.suggest_float('C', 1e-3, 100.0, log=True),\n",
    "            'penalty': trial.suggest_categorical('penalty', ['l2', 'l1']),\n",
    "            'solver': 'saga',\n",
    "            'max_iter': 1000,\n",
    "            'random_state': 42,\n",
    "            'class_weight': 'balanced'\n",
    "        }\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "def create_model(model_name: str, params: dict):\n",
    "    \"\"\"\n",
    "    Create model instance with given parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name : str\n",
    "        Name of the model\n",
    "    params : dict\n",
    "        Model parameters\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model : Model instance\n",
    "    \"\"\"\n",
    "    if model_name == 'RandomForest':\n",
    "        return RandomForestClassifier(**params)\n",
    "    elif model_name == 'XGBoost':\n",
    "        return XGBClassifier(**params)\n",
    "    elif model_name == 'GradientBoosting':\n",
    "        return GradientBoostingClassifier(**params)\n",
    "    elif model_name == 'LogisticRegression':\n",
    "        return LogisticRegression(**params)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "\n",
    "# Define models to compare\n",
    "MODELS_TO_COMPARE = {\n",
    "    'RandomForest': 'Tree-based ensemble with bagging',\n",
    "    'XGBoost': 'Gradient boosting with regularization',\n",
    "    'GradientBoosting': 'Sequential boosting algorithm',\n",
    "    'LogisticRegression': 'Linear baseline model'\n",
    "}\n",
    "\n",
    "print(\"Model Search Spaces Defined\")\n",
    "print(\"=\" * 60)\n",
    "for model_name, description in MODELS_TO_COMPARE.items():\n",
    "    print(f\"{model_name:20s} : {description}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tvwk9ly2wlg",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Run Nested Cross-Validation\n",
    "\n",
    "Execute nested CV for all models with Optuna hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "argjwnibuh5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested CV Execution with Optuna\n\ndef run_nested_cv_with_optuna(X: np.ndarray, \n                               y: np.ndarray,\n                               model_name: str,\n                               outer_cv: StratifiedKFold,\n                               inner_cv: StratifiedKFold,\n                               n_trials: int = 50,\n                               timeout: int = 300) -> Dict:\n    \"\"\"\n    Run nested cross-validation with Optuna hyperparameter tuning.\n    \n    Parameters:\n    -----------\n    X : np.ndarray\n        Feature matrix\n    y : np.ndarray\n        Target vector\n    model_name : str\n        Name of the model to train\n    outer_cv : StratifiedKFold\n        Outer CV splitter\n    inner_cv : StratifiedKFold\n        Inner CV splitter\n    n_trials : int\n        Number of Optuna trials per fold\n    timeout : int\n        Timeout in seconds per fold\n        \n    Returns:\n    --------\n    dict : Results dictionary\n    \"\"\"\n    print(f\"\\\n{'='*70}\")\n    print(f\"Running Nested CV: {model_name}\")\n    print(f\"{'='*70}\")\n    \n    outer_scores = []\n    best_params_per_fold = []\n    fold_predictions = []\n    \n    # Outer loop: Performance estimation\n    for fold_idx, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n        print(f\"\\\nOuter Fold {fold_idx + 1}/{outer_cv.n_splits}\")\n        print(\"-\" * 70)\n        \n        X_train_outer, X_test_outer = X[train_idx], X[test_idx]\n        y_train_outer, y_test_outer = y[train_idx], y[test_idx]\n        \n        # Scale features\n        scaler = StandardScaler()\n        X_train_outer = scaler.fit_transform(X_train_outer)\n        X_test_outer = scaler.transform(X_test_outer)\n        \n        # Inner loop: Hyperparameter tuning with Optuna\n        def objective(trial):\n            params = get_search_space(model_name, trial)\n            model = create_model(model_name, params)\n            \n            # Inner CV score\n            scores = cross_val_score(\n                model, X_train_outer, y_train_outer,\n                cv=inner_cv, scoring='f1_weighted', n_jobs=-1\n            )\n            \n            return scores.mean()\n        \n        # Create Optuna study\n        study = optuna.create_study(\n            direction='maximize',\n            sampler=TPESampler(seed=42),\n            pruner=HyperbandPruner()\n        )\n        \n        # Optimize\n        study.optimize(\n            objective,\n            n_trials=n_trials,\n            timeout=timeout,\n            show_progress_bar=False,\n            n_jobs=1  # Keep at 1 to avoid conflicts with CV parallelization\n        )\n        \n        best_params = study.best_params\n        best_params_per_fold.append(best_params)\n        \n        print(f\"  Best inner CV score: {study.best_value:.4f}\")\n        print(f\"  Best parameters: {best_params}\")\n        \n        # Train final model on full training set with best params\n        best_params_full = get_search_space(model_name, study.best_trial)\n        final_model = create_model(model_name, best_params_full)\n        final_model.fit(X_train_outer, y_train_outer)\n        \n        # Evaluate on held-out test set\n        y_pred = final_model.predict(X_test_outer)\n        fold_score = f1_score(y_test_outer, y_pred, average='weighted')\n        outer_scores.append(fold_score)\n        \n        print(f\"  Outer test F1 score: {fold_score:.4f}\")\n        \n        # Store predictions\n        fold_predictions.append({\n            'fold': fold_idx,\n            'y_true': y_test_outer,\n            'y_pred': y_pred,\n            'test_indices': test_idx\n        })\n    \n    # Summary\n    print(f\"\\\n{'='*70}\")\n    print(f\"Nested CV Results for {model_name}\")\n    print(f\"{'='*70}\")\n    print(f\"Mean F1 Score: {np.mean(outer_scores):.4f} \u00b1 {np.std(outer_scores):.4f}\")\n    print(f\"Fold scores: {[f'{s:.4f}' for s in outer_scores]}\")\n    print(f\"{'='*70}\")\n    \n    return {\n        'model_name': model_name,\n        'outer_scores': outer_scores,\n        'mean_score': np.mean(outer_scores),\n        'std_score': np.std(outer_scores),\n        'best_params_per_fold': best_params_per_fold,\n        'fold_predictions': fold_predictions\n    }\n\n# Run nested CV for all models\nif 'nested_cv' in locals() and 'X' in locals() and 'y' in locals():\n    outer_cv, inner_cv = nested_cv.get_cv_splits()\n    \n    all_results = {}\n    \n    for model_name in MODELS_TO_COMPARE.keys():\n        try:\n            results = run_nested_cv_with_optuna(\n                X, y,\n                model_name=model_name,\n                outer_cv=outer_cv,\n                inner_cv=inner_cv,\n                n_trials=10,  # Reduce for faster execution\n                timeout=60   # 3 minutes per fold\n            )\n            all_results[model_name] = results\n        except Exception as e:\n            print(f\"\\\nError with {model_name}: {e}\")\n            continue\n    \n    print(f\"\\\n\\\nCompleted nested CV for {len(all_results)} models!\")\nelse:\n    print(\"Nested CV framework not ready. Run previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ewutc9qw2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Model Comparison and Evaluation\n",
    "\n",
    "Compare all models and visualize performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dwic4pel8qa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison and Visualization\n\ndef compare_models(all_results: Dict) -> pd.DataFrame:\n    \"\"\"\n    Create comparison table of all models.\n    \n    Parameters:\n    -----------\n    all_results : dict\n        Results from all models\n        \n    Returns:\n    --------\n    pd.DataFrame : Comparison table\n    \"\"\"\n    comparison_data = []\n    \n    for model_name, results in all_results.items():\n        comparison_data.append({\n            'Model': model_name,\n            'Mean F1 Score': results['mean_score'],\n            'Std F1 Score': results['std_score'],\n            'Min Score': min(results['outer_scores']),\n            'Max Score': max(results['outer_scores']),\n            'CV Folds': len(results['outer_scores'])\n        })\n    \n    comparison_df = pd.DataFrame(comparison_data)\n    comparison_df = comparison_df.sort_values('Mean F1 Score', ascending=False)\n    \n    return comparison_df\n\ndef plot_model_comparison(all_results: Dict):\n    \"\"\"\n    Visualize model comparison.\n    \n    Parameters:\n    -----------\n    all_results : dict\n        Results from all models\n    \"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Plot 1: Bar plot with error bars\n    ax1 = axes[0]\n    model_names = list(all_results.keys())\n    mean_scores = [all_results[m]['mean_score'] for m in model_names]\n    std_scores = [all_results[m]['std_score'] for m in model_names]\n    \n    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(model_names)))\n    bars = ax1.bar(model_names, mean_scores, yerr=std_scores, \n                   capsize=5, color=colors, alpha=0.8, edgecolor='black')\n    \n    ax1.set_ylabel('F1 Score (Weighted)', fontsize=12, fontweight='bold')\n    ax1.set_title('Model Performance Comparison\\\n(Nested CV Results)', \n                  fontsize=14, fontweight='bold')\n    ax1.set_ylim([0, 1.0])\n    ax1.grid(True, alpha=0.3, axis='y')\n    ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random baseline')\n    \n    # Rotate x labels\n    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n    \n    # Plot 2: Box plot of fold scores\n    ax2 = axes[1]\n    fold_scores_list = [all_results[m]['outer_scores'] for m in model_names]\n    \n    bp = ax2.boxplot(fold_scores_list, labels=model_names, patch_artist=True,\n                     notch=True, showmeans=True)\n    \n    # Color boxes\n    for patch, color in zip(bp['boxes'], colors):\n        patch.set_facecolor(color)\n        patch.set_alpha(0.8)\n    \n    ax2.set_ylabel('F1 Score (Weighted)', fontsize=12, fontweight='bold')\n    ax2.set_title('Distribution of Fold Scores\\\n(Nested CV)', \n                  fontsize=14, fontweight='bold')\n    ax2.set_ylim([0, 1.0])\n    ax2.grid(True, alpha=0.3, axis='y')\n    ax2.axhline(y=0.5, color='red', linestyle='--', alpha=0.5)\n    \n    # Rotate x labels\n    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n    \n    plt.tight_layout()\n    plt.show()\n\ndef plot_confusion_matrices(all_results: Dict, class_names: List[str] = None):\n    \"\"\"\n    Plot confusion matrices for all models.\n    \n    Parameters:\n    -----------\n    all_results : dict\n        Results from all models\n    class_names : list\n        Names of classes\n    \"\"\"\n    n_models = len(all_results)\n    fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 5))\n    \n    if n_models == 1:\n        axes = [axes]\n    \n    for idx, (model_name, results) in enumerate(all_results.items()):\n        # Aggregate predictions across all folds\n        all_y_true = []\n        all_y_pred = []\n        \n        for fold_pred in results['fold_predictions']:\n            all_y_true.extend(fold_pred['y_true'])\n            all_y_pred.extend(fold_pred['y_pred'])\n        \n        # Calculate confusion matrix\n        cm = confusion_matrix(all_y_true, all_y_pred)\n        \n        # Normalize\n        cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        \n        # Plot\n        ax = axes[idx]\n        im = ax.imshow(cm_norm, interpolation='nearest', cmap='Blues')\n        ax.set_title(f'{model_name}\\\nF1={results[\"mean_score\"]:.3f}', \n                     fontsize=12, fontweight='bold')\n        \n        # Colorbar\n        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n        \n        # Labels\n        if class_names is not None:\n            tick_marks = np.arange(len(class_names))\n            ax.set_xticks(tick_marks)\n            ax.set_yticks(tick_marks)\n            ax.set_xticklabels(class_names, rotation=45, ha='right')\n            ax.set_yticklabels(class_names)\n        \n        # Add text annotations\n        thresh = cm_norm.max() / 2.\n        for i in range(cm_norm.shape[0]):\n            for j in range(cm_norm.shape[1]):\n                ax.text(j, i, f'{cm[i, j]}\\\n({cm_norm[i, j]:.2f})',\n                       ha=\"center\", va=\"center\",\n                       color=\"white\" if cm_norm[i, j] > thresh else \"black\",\n                       fontsize=9)\n        \n        ax.set_ylabel('True Label', fontsize=10)\n        ax.set_xlabel('Predicted Label', fontsize=10)\n    \n    plt.suptitle('Confusion Matrices (Aggregated Across All Folds)', \n                 fontsize=14, fontweight='bold', y=1.02)\n    plt.tight_layout()\n    plt.show()\n\n# Compare and visualize results\nif 'all_results' in locals() and len(all_results) > 0:\n    print(\"\\\n\" + \"=\"*80)\n    print(\"MODEL COMPARISON SUMMARY\")\n    print(\"=\"*80)\n    \n    comparison_df = compare_models(all_results)\n    print(comparison_df.to_string(index=False))\n    \n    print(\"\\\n\" + \"=\"*80)\n    \n    # Plot comparisons\n    plot_model_comparison(all_results)\n    \n    # Plot confusion matrices\n    unique_classes = np.unique(y)\n    class_names = [f'Class {c}' for c in unique_classes]\n    plot_confusion_matrices(all_results, class_names)\n    \nelse:\n    print(\"No results available. Run nested CV first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77iv6v1ee6u",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Feature Importance Analysis\n",
    "\n",
    "Understand which plasmid features are most important for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3nx8e2g0ynw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "\n",
    "def train_final_model_for_interpretation(X: np.ndarray, y: np.ndarray, \n",
    "                                          model_name: str, best_params: dict):\n",
    "    \"\"\"\n",
    "    Train a final model on full dataset for interpretation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray\n",
    "        Feature matrix\n",
    "    y : np.ndarray\n",
    "        Target vector\n",
    "    model_name : str\n",
    "        Model name\n",
    "    best_params : dict\n",
    "        Best parameters from nested CV\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model : Trained model\n",
    "    \"\"\"\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Create and train model\n",
    "    model = create_model(model_name, best_params)\n",
    "    model.fit(X_scaled, y)\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "def plot_feature_importance(model, feature_names: List[str], \n",
    "                            model_name: str, top_n: int = 20):\n",
    "    \"\"\"\n",
    "    Plot feature importances for tree-based models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : trained model\n",
    "        Model with feature_importances_ attribute\n",
    "    feature_names : list\n",
    "        Names of features\n",
    "    model_name : str\n",
    "        Model name for title\n",
    "    top_n : int\n",
    "        Number of top features to display\n",
    "    \"\"\"\n",
    "    if not hasattr(model, 'feature_importances_'):\n",
    "        print(f\"{model_name} does not have feature_importances_ attribute\")\n",
    "        return\n",
    "    \n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1][:top_n]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.barh(range(top_n), importances[indices], \n",
    "             color=plt.cm.viridis(np.linspace(0.3, 0.9, top_n)))\n",
    "    \n",
    "    plt.yticks(range(top_n), [feature_names[i] for i in indices])\n",
    "    plt.xlabel('Feature Importance', fontsize=12, fontweight='bold')\n",
    "    plt.title(f'Top {top_n} Feature Importances: {model_name}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(True, alpha=0.3, axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top features\n",
    "    print(f\"\\\nTop {top_n} features for {model_name}:\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, idx in enumerate(indices[:top_n]):\n",
    "        print(f\"{i+1:2d}. {feature_names[idx]:30s} : {importances[idx]:.4f}\")\n",
    "\n",
    "def analyze_feature_importance_across_models(all_results: Dict, X: np.ndarray, \n",
    "                                              y: np.ndarray, feature_names: List[str]):\n",
    "    \"\"\"\n",
    "    Analyze feature importance across all tree-based models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_results : dict\n",
    "        Results from nested CV\n",
    "    X : np.ndarray\n",
    "        Feature matrix\n",
    "    y : np.ndarray\n",
    "        Target vector\n",
    "    feature_names : list\n",
    "        Feature names\n",
    "    \"\"\"\n",
    "    tree_models = ['RandomForest', 'XGBoost', 'GradientBoosting']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n",
    "    \n",
    "    for idx, model_name in enumerate(tree_models):\n",
    "        if model_name not in all_results:\n",
    "            continue\n",
    "        \n",
    "        # Get best parameters from first fold (representative)\n",
    "        best_params = all_results[model_name]['best_params_per_fold'][0]\n",
    "        \n",
    "        # Train final model\n",
    "        model, scaler = train_final_model_for_interpretation(X, y, model_name, best_params)\n",
    "        \n",
    "        # Get importances\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1][:15]\n",
    "        \n",
    "        # Plot\n",
    "        ax = axes[idx]\n",
    "        ax.barh(range(15), importances[indices],\n",
    "               color=plt.cm.viridis(np.linspace(0.3, 0.9, 15)))\n",
    "        ax.set_yticks(range(15))\n",
    "        ax.set_yticklabels([feature_names[i][:25] for i in indices], fontsize=9)\n",
    "        ax.set_xlabel('Importance', fontsize=10, fontweight='bold')\n",
    "        ax.set_title(f'{model_name}', fontsize=12, fontweight='bold')\n",
    "        ax.invert_yaxis()\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.suptitle('Feature Importance Comparison Across Models', \n",
    "                 fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate feature importance consensus\n",
    "    print(\"\\\n\" + \"=\"*80)\n",
    "    print(\"FEATURE IMPORTANCE CONSENSUS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    importance_matrix = []\n",
    "    for model_name in tree_models:\n",
    "        if model_name not in all_results:\n",
    "            continue\n",
    "        best_params = all_results[model_name]['best_params_per_fold'][0]\n",
    "        model, _ = train_final_model_for_interpretation(X, y, model_name, best_params)\n",
    "        importance_matrix.append(model.feature_importances_)\n",
    "    \n",
    "    if len(importance_matrix) > 0:\n",
    "        # Average importances across models\n",
    "        avg_importances = np.mean(importance_matrix, axis=0)\n",
    "        top_indices = np.argsort(avg_importances)[::-1][:15]\n",
    "        \n",
    "        print(\"\\\nTop 15 features (averaged across all tree models):\")\n",
    "        print(\"-\" * 80)\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            print(f\"{i+1:2d}. {feature_names[idx]:35s} : {avg_importances[idx]:.4f}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "if 'all_results' in locals() and len(all_results) > 0:\n",
    "    if 'nested_cv' in locals():\n",
    "        feature_names = nested_cv.feature_names\n",
    "        \n",
    "        # Analyze across models\n",
    "        analyze_feature_importance_across_models(all_results, X, y, feature_names)\n",
    "        \n",
    "        # Individual model plots\n",
    "        best_model_name = max(all_results.items(), key=lambda x: x[1]['mean_score'])[0]\n",
    "        print(f\"\\\n\\\nDetailed analysis for best model: {best_model_name}\")\n",
    "        \n",
    "        if best_model_name in ['RandomForest', 'XGBoost', 'GradientBoosting']:\n",
    "            best_params = all_results[best_model_name]['best_params_per_fold'][0]\n",
    "            model, _ = train_final_model_for_interpretation(X, y, best_model_name, best_params)\n",
    "            plot_feature_importance(model, feature_names, best_model_name, top_n=25)\n",
    "else:\n",
    "    print(\"No results available for feature importance analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e73599n763",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Final Summary and Interpretation\n",
    "\n",
    "Summarize ML findings and biological insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ya825tjjz4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final ML Summary\n",
    "\n",
    "def generate_ml_summary(all_results: Dict, comparison_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Generate comprehensive ML summary report.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    all_results : dict\n",
    "        All model results\n",
    "    comparison_df : pd.DataFrame\n",
    "        Model comparison table\n",
    "    \"\"\"\n",
    "    print(\"\\\n\" + \"=\"*80)\n",
    "    print(\"NESTED CROSS-VALIDATION: FINAL SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Best model\n",
    "    best_model = comparison_df.iloc[0]\n",
    "    print(f\"\\\n\ud83c\udfc6 BEST MODEL: {best_model['Model']}\")\n",
    "    print(f\"   Mean F1 Score: {best_model['Mean F1 Score']:.4f} \u00b1 {best_model['Std F1 Score']:.4f}\")\n",
    "    print(f\"   Score Range: [{best_model['Min Score']:.4f}, {best_model['Max Score']:.4f}]\")\n",
    "    \n",
    "    # Performance across models\n",
    "    print(f\"\\\n\ud83d\udcca MODEL PERFORMANCE SUMMARY:\")\n",
    "    print(\"-\" * 80)\n",
    "    for _, row in comparison_df.iterrows():\n",
    "        print(f\"   {row['Model']:20s}: {row['Mean F1 Score']:.4f} \u00b1 {row['Std F1 Score']:.4f}\")\n",
    "    \n",
    "    # Hyperparameter insights\n",
    "    print(f\"\\\n\ud83d\udd27 HYPERPARAMETER INSIGHTS (Best Model):\")\n",
    "    print(\"-\" * 80)\n",
    "    best_model_name = best_model['Model']\n",
    "    if best_model_name in all_results:\n",
    "        params_per_fold = all_results[best_model_name]['best_params_per_fold']\n",
    "        \n",
    "        # Aggregate parameter values across folds\n",
    "        param_keys = params_per_fold[0].keys()\n",
    "        for param in param_keys:\n",
    "            values = [fold_params[param] for fold_params in params_per_fold \n",
    "                     if param in fold_params]\n",
    "            if len(values) > 0:\n",
    "                if isinstance(values[0], (int, float)):\n",
    "                    print(f\"   {param:25s}: {np.mean(values):.3f} \u00b1 {np.std(values):.3f}\")\n",
    "                else:\n",
    "                    # Categorical parameter\n",
    "                    most_common = max(set(values), key=values.count)\n",
    "                    print(f\"   {param:25s}: {most_common}\")\n",
    "    \n",
    "    # Statistical significance\n",
    "    print(f\"\\\n\ud83d\udcc8 STATISTICAL ANALYSIS:\")\n",
    "    print(\"-\" * 80)\n",
    "    if len(comparison_df) >= 2:\n",
    "        best_scores = all_results[comparison_df.iloc[0]['Model']]['outer_scores']\n",
    "        second_scores = all_results[comparison_df.iloc[1]['Model']]['outer_scores']\n",
    "        \n",
    "        # Simple t-test (paired if same folds)\n",
    "        from scipy.stats import ttest_rel\n",
    "        t_stat, p_value = ttest_rel(best_scores, second_scores)\n",
    "        \n",
    "        print(f\"   Comparison: {comparison_df.iloc[0]['Model']} vs {comparison_df.iloc[1]['Model']}\")\n",
    "        print(f\"   Paired t-test: t={t_stat:.3f}, p={p_value:.4f}\")\n",
    "        if p_value < 0.05:\n",
    "            print(f\"   \u2713 Difference is statistically significant (p < 0.05)\")\n",
    "        else:\n",
    "            print(f\"   \u2717 Difference is NOT statistically significant (p \u2265 0.05)\")\n",
    "    \n",
    "    # Biological interpretation\n",
    "    print(f\"\\\n\ud83e\uddec BIOLOGICAL INTERPRETATION:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"   The nested CV framework successfully predicted plasmid LIN bins\")\n",
    "    print(f\"   from sequence-derived features, demonstrating that:\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   1. Plasmid family relationships can be learned from composition\")\n",
    "    print(f\"   2. k-mer frequencies capture phylogenetic signals\")\n",
    "    print(f\"   3. GC content and skew are informative for plasmid typing\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   This validates the pLIN system's biological relevance and\")\n",
    "    print(f\"   suggests that sequence features alone can approximate\")\n",
    "    print(f\"   the full ANI-based LIN code calculation.\")\n",
    "    \n",
    "    # Next steps\n",
    "    print(f\"\\\n\ud83c\udfaf NEXT STEPS:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"   1. Validate on external plasmid datasets (PLSDB, NCBI)\")\n",
    "    print(f\"   2. Incorporate gene annotation features (MOB-suite, AMRFinder)\")\n",
    "    print(f\"   3. Implement real-time prediction API for plasmid typing\")\n",
    "    print(f\"   4. Compare with existing methods (pMLST, MOB typing)\")\n",
    "    print(f\"   5. Publish pLIN system and ML models\")\n",
    "    \n",
    "    print(f\"\\\n\" + \"=\"*80)\n",
    "\n",
    "# Generate summary\n",
    "if 'all_results' in locals() and 'comparison_df' in locals():\n",
    "    generate_ml_summary(all_results, comparison_df)\n",
    "    \n",
    "    print(f\"\\\n\\\n\u2705 Nested Cross-Validation Analysis Complete!\")\n",
    "    print(f\"\\\nThis rigorous ML analysis demonstrates that the plasmid LIN system\")\n",
    "    print(f\"is not only biologically meaningful but also computationally predictable.\")\n",
    "else:\n",
    "    print(\"Run all previous cells to generate the complete ML summary.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plasmid-lin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}